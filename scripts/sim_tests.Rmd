---
title: "simulation"
output: pagedown::book_crc
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r}
library(tidyverse)
library(phyloseq)
library(DESeq2)
library(microTF)
theme_set(theme_bw())
set.seed(20230317)
```

This generates the coefficient matrices for dynamics, perturbations, and
interactions.

```{r}
n_subject <- 40
n_time <- 30
n_lag <- 5
n_taxa <- 100
n_perturb <- 1
n_latent <- 2
n_covariates <- 3
prop_nonnull <- 0.1

A <- low_rank_step(n_taxa, n_latent, n_taxa, n_lag) |>
 sparsify() |>
 normalize(0.8)
B <- low_rank_step(n_taxa, n_latent, n_perturb, n_lag, lower = -1.5, upper = 1.5)

nonnull_taxa <- sample(n_taxa, prop_nonnull * n_taxa)
for (i in seq_along(B)) {
  B[[i]][-nonnull_taxa, ] <- 0
}

C <- low_rank_step_(n_taxa, n_latent, n_perturb, n_lag)
for (i in seq_along(C)) {
  C[[i]] <- C[[i]] |>
    sparsify() |>
    normalize(0.2)
}
```

Now we can simulate one example series.

For negative binomial simulation, remember that if the variance is $\mu$, the
variance is $\mu + \frac{1}{\text{size}}\mu^2$, where $\text{size}$ is the
number of successes before we stop.

```{r}
step_generator <- step_t(linear_sum(A), linear_sum(B), interaction_sum(C))
theta0 <- matrix(2, n_taxa, n_lag + 1)

w <- replicate(n_subject, matrix(0, n_perturb, n_time), simplify = FALSE)
for (i in seq_along(w)) {
  for (j in seq_len(n_perturb)) {
    start_ix <- sample((n_time / 3) : (n_time - n_lag), 1)
    end_ix <- start_ix + sample((n_lag / 2) : (2 * n_lag), 1)
    w[[i]][j, start_ix:min(n_time, end_ix)] <- 1
  }
}

z <- matnorm(n_subject, n_covariates)
sizes <- runif(n_taxa, .1, 10)
baselines <- rgamma(n_taxa, 40, 4)

x <- list()
for (i in seq_len(n_subject)) {
  x[[i]] <- generate_sample(theta0, w[[i]], z[i, ], step_generator, nbinom_sampler(sizes, baselines))
  rownames(x[[i]]) <- str_c("tax", seq_len(n_taxa))
}
```

Next, we can plot the series.

```{r}
x_df <- map_dfr(x, ~ as_tibble(.) |> rownames_to_column("taxon"), .id = "subject") |>
  pivot_longer(starts_with("V"), names_to = "time") |>
  mutate(
    time = as.integer(str_remove(time, "V")),
    taxon = str_c("tax", taxon)
  ) |>
  filter(time  > n_lag)

B_df <- map_dfr(
  B, ~ as_tibble(.) |> 
    mutate(taxon = str_c("tax", row_number())),
  .id = "lag") |>
  pivot_wider(names_from = "lag", values_from = "V1")

x_df <- x_df |>
  left_join(B_df) %>%
  mutate(effect = rowSums(.[, -c(1:4)]))
  
start_ix <- min(which(w[[2]][1, ] == 1))
end_ix <- max(which(w[[2]][1, ] == 1))

x_df |>
  filter(effect != 0, subject == 2) |>
  ggplot(aes(time, value)) +
  geom_rect(xmin = start_ix, xmax = end_ix, ymin = 0, ymax = max(x_df$value), fill = "#d3d3d3") +
  geom_line(aes(col = effect, group = subject)) +
  facet_wrap(~ reorder(taxon, effect), scale = "free_y") +
  scale_y_sqrt() +
  scale_color_gradient2(mid = "#d3d3d3")
```


```{r}
w_df <- map_dfr(w, ~ as_tibble(.), .id = "subject") |>
  pivot_longer(-subject, names_to = "time", values_to = "w") |>
  mutate(
    time = as.integer(str_remove(time, "V")),
    value = w,
    sample = str_c("sam", row_number())
  )

w_df_ <- w_df |>
  filter(w != 0, time > n_lag)

x_df |>
  filter(taxon == str_c("tax", tail(nonnull_taxa, 1))) |>
  ggplot(aes(time, reorder(subject, value))) +
  geom_tile(aes(fill = log(1 + value))) +
  geom_point(data = w_df_, shape = 1) +
  scale_fill_distiller(direction = 1)
```

### DESeq2

If we were using DESeq2, how would we test?

```{r}
reads <- map_dfc(x, ~ .[, -seq_len(n_lag)]) |>
  as.matrix()
rownames(reads) <- str_c("tax", seq_len(n_taxa))
colnames(reads) <- w_df |>
  filter(time > n_lag) |>
  pull(sample)

ps <- phyloseq(
  otu_table(reads, taxa_are_rows = TRUE),
  sample_data(select(w_df, -value) |> column_to_rownames("sample"))
)

dds  <- phyloseq_to_deseq2(ps, ~ w)
dds <- DESeq(dds)
```

```{r}
significant <- results(dds) |>
  data.frame() |>
  rownames_to_column("taxon") |>
  filter(padj < 0.2)

significant |>
  arrange(padj)
```

These are the taxa that DESeq2 thinks are significant. We've ordered by
estimated log fold change and colored by the true perturbation effect.

```{r}
x_df |>
  right_join(select(significant, taxon, log2FoldChange, padj) |> filter(padj < 1e-6)) |>
  filter(subject == 1) |>
  ggplot() +
  geom_rect(xmin = start_ix, xmax = end_ix, ymin = 0, ymax = max(x_df$value), fill = "#d3d3d3") +
  geom_line(aes(time, value, col = effect)) +
  facet_wrap(~ reorder(taxon, log2FoldChange)) +
  scale_y_log10() +
  scale_color_gradient2(mid = "#d3d3d3")
```

```{r}
nonnull_taxa_ <- str_c("tax", nonnull_taxa)

R <- significant$taxon
S <- intersect(nonnull_taxa_,  significant$taxon)
V <- setdiff(significant$taxon, nonnull_taxa_)
```

At a $q$-level of 0.2, the FDP is `r length(V) / length(R)`. The power is 
`r length(S) / nrow(significant)`. There are a lot of false positives.

### Mirror-Based Testing

```{r}
subject_data <- z |>
  as_tibble() |>
  mutate(subject = row_number())

metadata <- expand.grid(
    time = seq_len(n_time),
    subject = seq_len(n_subject)
  ) |>
  mutate(sample = str_c("sam", row_number())) |>
  filter(time > n_lag)

interventions <- w_df |>
  filter(time > n_lag) |>
  pull(w, sample) |>
  as.matrix()

reads <- log(1 + reads)
ts <- ts_from_dfs(t(reads), interventions, metadata, subject_data)
```

Now, how can we get the y1 and y0 for each person across the two splits? Note
that we don't have to apply the model to the test split (unlike everything else
we've been taught) -- it's going to be enough to ensure sign consistency across
splits.

```{r}
pd_generator <- function(ts, lags) {
  x <- patchify_df(ts, lags[1], lags[2])$x
  
  function(fit, partial_x) {
    pnames <- intersect(colnames(partial_x), colnames(x))
    nx <- nrow(partial_x)
    y_hat <- matrix(nrow = length(fit), ncol = nx)

    for (j in seq_along(fit)) {
      x_ <- x
 
      for (i in seq_len(nx)) {
        x_[, pnames] <- partial_x[i, ]
        y_hat[j, i] <- mean(predict(fit[[j]], x_))
      }
    }
    
    y_hat
  }
}
```

```{r}
w0 <- matrix(0, n_lag, n_lag)
colnames(w0) <- str_c("intervention1_lag", seq(0, n_lag -1 ))
w1 <- w0
for (i in seq_len(n_lag)) {
  w1[i, ] <- c(rep(1, i), rep(0, n_lag - i))
}
```


```{r}
pd_effects <- function(pd, fit, w0, w1) {
  pd(fit, w1) - pd(fit, w0)
}

pd_splits <- function(ts, w0, w1, n_splits, method = "gbm", ...) {
  effects <- replicate(n_splits, array(dim = c(nrow(ts[[1]]), 2, nrow(w0))), simplify = FALSE)

  for (s in seq_len(n_splits)) {
    print(str_c("Evaluating split ", s))
    split_ix <- sample(length(ts), 0.5 * length(ts))
    fits <- list(
      train(ts[split_ix], method = "gbm", ...)@parameters,
      train(ts[-split_ix], method = "gbm", ...)@parameters
    )

    lags <- microTF:::time_lags(fits[[1]][[1]])
    pd_fun <- list(
      pd_generator(ts[split_ix], lags),
      pd_generator(ts[-split_ix], lags)
    )
    
    for (i in seq_along(fits)) {
      effects[[s]][,i,] <- pd_effects(pd_fun[[i]], fits[[i]], w0, w1)
    }
  }
  
  effects
}
```

```{r}
consistency_mirror <- function(effects) {
  sgn <- sign(effects[, 1] * effects[, 2])
  magnitude <- rowMeans(abs(effects))
  sgn * magnitude
}

effects <- pd_splits(ts, w0, w1, 10, method = "gbm")
```


```{r}
ms <- list()
k <- 1
for (s in seq_along(effects)) {
  for (lag in seq_len(dim(effects[[s]])[3])) {
    ms[[k]] <- tibble(
      m = consistency_mirror(effects[[s]][,, lag]),
      lag = lag,
      multisplit = s
    ) |>
      mutate(taxon = row_number())
    k <- k + 1
  }
}

ms <- bind_rows(ms) |>
  mutate(
    truth = taxon %in% nonnull_taxa,
    m_ = m + rnorm(n(), 0, 0.1)
  )

ggplot(ms) +
  geom_histogram(aes(m_, fill = truth)) +
  facet_wrap(~ lag)
```

```{r}
ms_ <- ms |>
  select(multisplit, m_, lag) %>% 
  split(.$lag) %>%
  map(~ split(., .$multisplit) %>% map(~ pull(., m_)))

lag <- 1
R <- str_c("tax", which(multiple_data_splitting(ms_[[lag]])))
S <- intersect(nonnull_taxa_,  R)
V <- setdiff(R, nonnull_taxa_)

length(V) / length(R)
length(S) / length(nonnull_taxa_)
```
