---
title: "Mirror Example"
output: pagedown::book_crc
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, out.width = 400, fig.height = 4, fig.width = 8)
```

```{r}
library(tidyverse)
theme_set(theme_bw())
set.seed(20230315)
```

We'll use these functions.

```{r}
sample_m <- function(n0, n1, mu = 2) {
  tibble(
    m = c(rnorm(n0), rnorm(n1, mu)),
    nonnull = c(rep(FALSE, n0), rep(TRUE, n1))
  )
}

fdp_hat <- function(m) {
  m_sort <- sort(abs(m))
  fdp <- tibble(t = m_sort, fdp = 0)
  
  for (j in seq_along(m_sort)) {
    fdp$fdp[j] <- sum(m < -m_sort[j]) / sum(m > m_sort[j])
  }
  
  fdp
}

tau_q <- function(fdp, q) {
  fdp |>
    filter(fdp < q) |>
    slice_min(t) |>
    pull(t)
}
```

```{r}
N <- 500
P <- 250
S0 <- 50
b <- 3
x <- scale(matrix(rnorm(N * P), N, P)) / sqrt(P)
beta <- c(rep(b, S0), rep(0, P - S0))
y <- x %*% beta + rnorm(N)
ix <- sample(nrow(x), nrow(x) / 2)

xy <- cbind(y, x[, c(1:2, (S0 + 1):(S0 + 2))]) |>
  as_tibble() |>
  rownames_to_column("sample") |>
  mutate(split = row_number() %in% ix) |>
  pivot_longer(V2:V5)

ggplot(xy, aes(value, V1)) +
  geom_hline(yintercept = 0, col = "#d3d3d3", size = 2) +
  geom_point(size = 0.5, alpha = 0.6) +
  stat_smooth(method = "lm", se = FALSE) +
  coord_fixed(0.02) +
  facet_wrap(~ name)
ggsave("~/Desktop/laboratory/talks/2023/20230414/figures/scatter_mirror_toy.png")

pal <- c("#6F96A6", "#684F8C")
ggplot(xy, aes(value, V1, col = split)) +
  geom_hline(yintercept = 0, col = "#d3d3d3", size = 2) +
  scale_color_manual(values = pal) +
  geom_point(size = 0.5, alpha = 0.6) +
  stat_smooth(method = "lm", se = FALSE) +
  coord_fixed(0.02) +
  facet_wrap(~ name) +
  theme(legend.position = "non")

ggsave("~/Desktop/laboratory/talks/2023/20230414/figures/scatter_mirror_toy_col.png")
```

```{r}
library(glmnet)
fits <- list(
  glmnet(x[ix, ], y[ix], alpha = .1, lambda = .05),
  glmnet(x[-ix, ], y[-ix], alpha = .1, lambda = .05)
)

b_splits <- map_dfr(
  fits, ~ {
    b <- as.matrix(coef(.))
    b_df <- data.frame(b = b[, 1])
    b_df$term <- rownames(b)
    b_df
  }
  , .id = "split")

pal <- c("#6F96A6", "#684F8C")
ggplot(b_splits) +
  geom_point(aes(reorder(term, -abs(b)), b, col = split)) +
  labs(y = expression(hat(beta[j])^{(s)}), x = "Sorted Features") +
  scale_color_manual(values = pal) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title = element_text(size = 16),
    panel.grid = element_blank()
  )
ggsave("~/Desktop/laboratory/talks/2023/20230414/figures/mirror_betas.png", width = 8, height = 4)
```

Here are some idealized mirror statistics.

```{r}
library(glue)
m <- b_splits |>
  pivot_wider(names_from = split, values_from = b) |>
  mutate(
    m = sign(`1` * `2`) * (`1` + `2`),
    nonnull = term %in% glue("V{1:S0}")
  )

ggplot(m) +
  geom_histogram(aes(m, fill = nonnull), position = "identity", alpha = 0.6) +
  scale_fill_manual(values = pal) +
  labs(x = expression(M[j]), y = "Count") +
  theme(
    axis.text = element_text(size = 14),
    legend.title = element_text(size = 16),
    legend.text = element_text(size = 16),
    axis.title = element_text(size = 20)
  )
ggsave("~/Desktop/laboratory/talks/2023/20230414/figures/mirror_histogram.png", width = 8, height = 6)
```

We can compute an FDR estimator from them.

```{r}
fdp <- fdp_hat(m$m)
tau <- tau_q(fdp, 0.2)

ggplot(fdp) +
  geom_point(aes(t, fdp)) +
  geom_vline(xintercept = tau) +
  geom_hline(yintercept = 0.2) +
  labs(x = expression(t), y = expression(widehat(FDP)(t))) +
  theme(
    axis.text = element_text(size = 14),
    legend.title = element_text(size = 16),
    legend.text = element_text(size = 16),
    axis.title = element_text(size = 20)
  )
  
ggsave("~/Desktop/laboratory/talks/2023/20230414/figures/fdp_hat.png", width = 8, height = 6)
```

```{r}
selections <- function(m, tau) {
  m > tau
}

s_hat <- selections(m, tau)
m <- m |>
  mutate(nonnull_hat = s_hat)

errors <- m |>
  count(nonnull, nonnull_hat)

errors
errors$n[2] / (errors$n[2] + errors$n[4]) # actual FDP
errors$n[4] / (errors$n[3] + errors$n[4]) # actual power
```

If we use multiple data splitting, then we supposedly can improve power. We'll
imagine this by creating many different m's.

```{r}
inclusion <- function(s_hat) {
  s_hat <- 1.0 * s_hat
  colMeans(s_hat / rowSums(s_hat))
}

consolidate <- function(s_hat, q) {
  I_hat <- inclusion(s_hat)
  ix <- order(I_hat)
  I_sort <- cumsum(I_hat[ix])
  j_star <- max(which(I_sort <= q))
  I_hat > I_hat[ix[j_star]]
}

multiple_data_splitting <- function(ms, q) {
  s_hat <- matrix(FALSE, length(ms), length(ms[[1]]))
  
  for (k in seq_along(ms)) {
   fdp <- fdp_hat(ms[[k]])
   tau <- tau_q(fdp, q)
   s_hat[k, ] <- selections(ms[[k]], tau)
  }
  
  consolidate(s_hat, q)
}

ms <- replicate(20, sample_m(n0, n1), simplify = FALSE)
s_hat <- multiple_data_splitting(map(ms, ~ .$m), 0.1)

m <- m |>
  mutate(nonnull_mds = s_hat)

errors <- m |>
  mutate(
    nonnull = factor(nonnull, levels = c("TRUE", "FALSE")),
    nonnull_mds = factor(nonnull_mds, levels = c("TRUE", "FALSE"))
  ) |>
  count(nonnull, nonnull_mds, .drop = FALSE)
```


```{r}
errors
errors$n[3] / (errors$n[3] + errors$n[4]) # actual FDP
errors$n[1] / (errors$n[1] + errors$n[2]) # actual power
```
